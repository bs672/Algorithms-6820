\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm, algpseudocode}
\usepackage{enumitem}
\newtheorem{lemma}{Lemma}

\newcommand{\astarinv}{$(A^*)^{-1}$}

\begin{document}
\begin{flushright}
Andrew Bennett\\
Richard Strong Bowen\\
Bhai Jaiveer Singh\\
Ivy Suiwen Wu\\
CS 6820 Problem Set 3
\end{flushright}
\section*{Problem 1}
\subsection*{1a}
\begin{lemma}
For an invertible $n\times n$ matrix $T$ with integral entries from $[-U, U]$,
$\text{det}(T)$ is an integer and $|\text{det}(T)| \leq (nU)^n$
\end{lemma}
\begin{proof}
We argue by induction. For a $1\times 1$ matrix $T = [u]$, we have
$|\text{det}(T)|= |u|\leq U$ and $|\text{det}(T)| = |u|$ which is an integer.
       
For the induction step, let $T$ be $(n+1)\times (n+1)$.
       
Then to prove the upper bound:
\begin{align*}
	\text{det}(T) &= \sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i} \quad \text{(Definition of determinant)}\\
	\left |\text{det}(T)\right |&\leq \left |\sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i}\right|\\
	&\leq \sum_{i=1}^{n}(nU)^{n}\cdot |T_{1i}|\quad \text{By induction hypothesis)}\\
&\leq(n+1)U\cdot (nU)^{n}\\
&\leq ((n+1)U)^{n+1}
\end{align*}
where $M_i$ is the $(1,i)$ matrix minor: the $n\times n$ matrix formed by removing row $1$ and column $i$ of $T$, which is itself integral.

To prove integrality note that
\[\text{det}(T) = \sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i}\]
expresses $T$ as the sum of products determinants of integral matrices --
integral by the induction hypothesis -- and elements of $T$, also
integral. Hence it is itself integral.

\end{proof}

Cramer's rule (todo: cite Strang) tells us that 
\[T^{-1} = \frac{1}{\text{det}(T)}C^{\intercal},\]
where $C$ is the cofactor matrix of $T$. Every element of $C$ is the
determinant of some matrix minor of $T$, which are all invertible integral
matrices. Therefore, the lemma tells us that the absolute value of elements of $C$ is bounded
between $[1, (nU)^{n}]$. From the lemma above, we also have
$\frac{1}{|\text{det}(T)|}$ is bounded between $[(nU)^{-n},1]$. This concludes
the proof that the absolute value of entries of $T^{-1}$ is bounded above by
$(nU)^{n}$ and the absolute value of non-zero entries of $T^{-1}$ is bounded
below by $(nU)^{-n}$.
\subsection*{1b}
Let $P = \{x \in \mathbb{R}^n | Ax \leqslant b\}$. We will construct $Q = \{y \in \mathbb{R}^{2n} | A'x \leqslant b'\} $ such that $Q$ is feasible if and only if $P$ is, by constructing $A'$ and $b'$. Let
\[ A' = \begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix}\]
(where $I_k$ means the $k \times k$ identity matrix) and
\[ b' = \begin{bmatrix} b \\ \vec0_{2n} \end{bmatrix} \] 
(where $\vec 0_k$ means the $k$-element all-zeros column vector). This transform is clearly polynomial-time, and ensures $Q$ is a subset of the positive orthant because of the last $2n$ constraints.

First we show that $P$ being feasible implies that $Q$ is. Suppose $x \in P$. We will produce a $y\in \mathbb{R}^{2n}$ which is in $Q$. Index $y$ as:
\[ y = \begin{bmatrix} y^+ \\ y^- \end{bmatrix} = \begin{bmatrix} y^+_1 \\ \vdots \\ y^+_n \\ y^-_1 \\ \vdots \\ y^-_n \end{bmatrix}\]
and set
\[y^+_i = \begin{cases} x_i & \quad \text {$x_i$ is nonnegative} \\ 0 & \quad \text {otherwise} \end{cases}\]
and
\[y^-_i = \begin{cases} -x_i & \quad \text {$x_i$ is negative} \\ 0 & \quad \text {otherwise} \end{cases}.\]
Note that $y \geqslant 0$ and $x = y^+ - y^-$. Then
\begin{align*}
A'y &= \begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix} \begin{bmatrix} y^+ \\ y^- \end{bmatrix} \\
&= \begin{bmatrix} Ax \\ -y \end{bmatrix} \\
&\leqslant \begin{bmatrix} b \\ \vec{0}_{2n} \end{bmatrix} \quad \text {(since $x \in P$ and $y$ is nonnegative)}\\
&= b'
\end{align*}
Hence $y \in Q$.

Second we show that $Q$ being feasible imples that $P$ is. Suppose $y\in Q$. We will produce an $x \in \mathbb{R}^{2n}$ which is in $P$. Index $y$ as above and let $x = y^+ - y^-$. Then we have this chain of implications
\begin{align*}
&\begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix} \begin{bmatrix} y^+ \\ y^- \end{bmatrix} \leqslant b' \quad \text{(since $y \in Q$)}\\
\implies & A(y^+ - y^-) \leqslant b \quad \text{(Ignoring the last $2n$ rows)}\\
\implies & Ax \leqslant b
\end{align*}
Hence $x \in Q$.

\subsection*{1c}

\begin{lemma}
  Suppose the polyhedron $P = \{x \in \mathbb{R_+}^n | Ax \leq b\}$ is non-empty,
  and there are at least as many constraints as variables. Then
  there is a solution $x \in P$ that is tight in at least $n$
  constraints, where $n$ is the number of variables, and where the
  corresponding rows of $A$ are linearly independent.
\end{lemma}
\begin{proof}
  TODO: RICHARD ADD ARGUMENT HERE
\end{proof}

We can first observe that we only need to prove the forward direction
($P$ is non-empty implies that $P \cap [0, R]^n$ is non-empty),
since the reverse is trivial. Suppose that $P$ is non-empty. First
we note that we can trivially ensure we have at least $n$ constraints
without changing $P$, by making sure that the non-negativity constraints
for all variables are explicitly included.

Given the above lemma,
there is a feasible solution $x^*$ that is tight in at least $n$
of the constraints. Let $A^*$ be the $n \times n$ matrix from the rows
of $A$ corresponding to $n$ such tight constraints (chosen
arbitrarily if there are more than $n$ tight constraints), and let
$b^*$ be the column vector from the corresponding rows of $b$.
Then by construction the following equations are satisfied:

\begin{align}
  A^* x^* &= b^* \\
  x^* &= (A^*)^{-1} b^* 
\end{align}

Now from the second equation above we can conclude that each element
of $x^*$ can be written as the sum of $n$ entries from \astarinv,
each multiplied by an integer in the range of $[-U, U]$. Since $A^*$
is an $n \times n$ matrix satisfying the conditions in the premise
of part (1a), we can conclude that the absolute values of the
entries in \astarinv are bounded above by $(nU)^n$. Therefore it
clearly follows that each entry in $x^*$ is bounded above by
$(nU)^n U n = (nU)^{n + 1}$.

Therefore if we set
$R = (nU)^{n + 1}$ it must be the case that if $P$ is
non-empty there is a solution in $P \cap [0, R]^n$.
Finally, we can observe that $\log(R) = (n + 1)\log(n) + (n + 1)\log(U)$
which is clearly in $poly(n\log(U))$, so we are done.


\subsection*{1d}
\subsection*{1e}\begin{algorithm}[H]
\caption{{\sc CheckFeasibility}}
\begin{algorithmic}[1]
\State Let {\em Separation Oracle} check $x\in P_{\epsilon}$
\If{$x \in P $}
\Return nothing
\Else
\State Let $a_j^{\intercal}$ denote the row vector returned by {\em Separation Oracle}
\State \Return $a_j^{\intercal}x = b_j$
\EndIf
\end{algorithmic}
\end{algorithm}
\textbf{Claim:} $P\cap \{x\in\mathbb{R}^n\mid a_j^{\intercal}x = b_j\}$ is non-empty.
\begin{proof}
  Define new polyhedron $P' = P\cap \{x\in\mathbb{R}^n\mid a_j^{\intercal}x = b_j\} =
  \{x\in\mathbb{R}^n\mid A'x \leq b, a_j^{\intercal}x \leq b_j, -a_j^{\intercal}x \leq -b_j \}$,
  where $A'$ is matrix $A$ without row vector $a_j^{\intercal}$ and its corresponding $P'_{\epsilon} = \{A'x\leq b + \vec{1}\epsilon, a_j^{\intercal}x \leq b_j + \epsilon, -a_j^{\intercal}x \leq -b_j + \epsilon\}$ where $\epsilon$ is defined in (1d) for $P$. \\
  Consider the vector $x$ under interest, $x\in P_{\epsilon}$ and {\em separation oracle}
  has verified that $x\notin P$ and outputted row vector $a_j^{\intercal}$ such that $a_j^{\intercal}x > b_j$. Therefore, $x\in P'_{\epsilon}$. According to (1d), since $P'_{\epsilon}$ is non-empty, we conclude that $P'$ is also non-empty. 
\end{proof}
\subsection*{1f}
%My idea for 1f is that we recursively use the algorithm in 1e, and every time
%either we find a point $x$ in $P$ or we can successfully decrease the dimension of the polyhedron by 1
%The part of the problem that I have not figured out is how to determine a
%point $x\in P_{\epsilon}$ to test. In Ellipsoid algorithm, the algorithm recursively
%takes the center of the new ellipse. I think it should work if we recursively test the center of the "cube" in the problem.
%Another aspect that we might need to touch on is how to reshape/scale to get a new
%"cube" after each iteration, which i personally don't think will be super hard.
%Should be similar to the shrink/stretch method for ellipsoid algorithm
\section*{Problem 2}
\subsection*{2a}
\subsection*{2b}
\end{document}
