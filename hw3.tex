\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm, algpseudocode}
\usepackage{enumitem}
\newtheorem{lemma}{Lemma}

\newcommand{\astarinv}{$(A^*)^{-1}$}
\newcommand{\constraintj}{$a_j^{\intercal}x \leq b_j$}

\begin{document}
\begin{flushright}
Andrew Bennett\\
Richard Strong Bowen\\
Bhai Jaiveer Singh\\
Ivy Suiwen Wu\\
CS 6820 Problem Set 3
\end{flushright}
\section*{Problem 1}
\subsection*{1a}
\begin{lemma}
For an invertible $n\times n$ matrix $T$ with integral entries from $[-U, U]$,
$\text{det}(T)$ is an integer and $|\text{det}(T)| \leq (nU)^n$
\end{lemma}
\begin{proof}
We argue by induction. For a $1\times 1$ matrix $T = [u]$, we have
$|\text{det}(T)|= |u|\leq U$ and $|\text{det}(T)| = |u|$ which is an integer.
       
For the induction step, let $T$ be $(n+1)\times (n+1)$.
       
Then to prove the upper bound:
\begin{align*}
	\text{det}(T) &= \sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i} \quad \text{(Definition of determinant)}\\
	\left |\text{det}(T)\right |&\leq \left |\sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i}\right|\\
	&\leq \sum_{i=1}^{n}(nU)^{n}\cdot |T_{1i}|\quad \text{(By induction hypothesis)}\\
&\leq(n+1)U\cdot (nU)^{n}\\
&\leq ((n+1)U)^{n+1}
\end{align*}
where $M_i$ is the $(1,i)$ matrix minor: the $n\times n$ matrix formed by removing row $1$ and column $i$ of $T$, which is itself integral.

To prove integrality note that either $\text{det}(M_i)$ is zero if the minor is not invertible, or it is an integer by the induction hypothesis. Thus
\[\text{det}(T) = \sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i}\]
expresses $\text{det}(T)$ as the sum of products integers. Thus it is integral.

\end{proof}

Cramer's rule (here we acknowledge "Introduction to Linear Algebra" by Strang) tells us that 
\[T^{-1} = \frac{1}{\text{det}(T)}C^{\intercal},\]
where $C$ is the cofactor matrix of $T$. Every element of $C$ is the
determinant of some matrix minor of $T$. There are two cases for an entry $C_{ij}$:

\begin{enumerate} 

\item The matrix minor is singular. Then $T^{-1}_{ij}$ is zero.
\item The matrix minor is not singular. Then the lemma tells us that $|\text{det}(C_ij)|$ is in
between $[1, (nU)^{n}]$. From the lemma above, we also have
$\frac{1}{|\text{det}(T)|}$ is bounded between $[(nU)^{-n},1]$. Thus the entry $T^{-1}_{ij}$ is nonzero and its absolute value is in the range
$[(nU)^{-n}, (nU)^{n}]$
\end{enumerate}
\subsection*{1b}
Let $P = \{x \in \mathbb{R}^n | Ax \leqslant b\}$. We will construct $Q = \{y \in \mathbb{R}^{2n} | A'x \leqslant b'\} $ such that $Q$ is feasible if and only if $P$ is, by constructing $A'$ and $b'$. Let
\[ A' = \begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix}\]
(where $I_k$ means the $k \times k$ identity matrix) and
\[ b' = \begin{bmatrix} b \\ \vec0_{2n} \end{bmatrix} \] 
(where $\vec 0_k$ means the $k$-element all-zeros column vector). This transform is clearly polynomial-time, and ensures $Q$ is a subset of the positive orthant because of the last $2n$ constraints.

First we show that $P$ being feasible implies that $Q$ is. Suppose $x \in P$. We will produce a $y\in \mathbb{R}^{2n}$ which is in $Q$. Index $y$ as:
\[ y = \begin{bmatrix} y^+ \\ y^- \end{bmatrix} = \begin{bmatrix} y^+_1 \\ \vdots \\ y^+_n \\ y^-_1 \\ \vdots \\ y^-_n \end{bmatrix}\]
and set
\[y^+_i = \begin{cases} x_i & \quad \text {$x_i$ is nonnegative} \\ 0 & \quad \text {otherwise} \end{cases}\]
and
\[y^-_i = \begin{cases} -x_i & \quad \text {$x_i$ is negative} \\ 0 & \quad \text {otherwise} \end{cases}.\]
Note that $y \geqslant 0$ and $x = y^+ - y^-$. Then
\begin{align*}
A'y &= \begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix} \begin{bmatrix} y^+ \\ y^- \end{bmatrix} \\
&= \begin{bmatrix} Ax \\ -y \end{bmatrix} \\
&\leqslant \begin{bmatrix} b \\ \vec{0}_{2n} \end{bmatrix} \quad \text {(since $x \in P$ and $y$ is nonnegative)}\\
&= b'
\end{align*}
Hence $y \in Q$.

Second we show that $Q$ being feasible imples that $P$ is. Suppose $y\in Q$. We will produce an $x \in \mathbb{R}^{2n}$ which is in $P$. Index $y$ as above and let $x = y^+ - y^-$. Then we have this chain of implications
\begin{align*}
&\begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix} \begin{bmatrix} y^+ \\ y^- \end{bmatrix} \leqslant b' \quad \text{(since $y \in Q$)}\\
\implies & A(y^+ - y^-) \leqslant b \quad \text{(Ignoring the last $2n$ rows)}\\
\implies & Ax \leqslant b
\end{align*}
Hence $x \in Q$.

\subsection*{1c}

\begin{lemma}
  Suppose the polyhedron $P = \{x \in \mathbb{R_+}^n | Ax \leq b\}$ is
non-empty, then there is a feasible $x^* \in P$ where the submatrix of $A$ corresponding to the tight constraints at $x^*$ is rank-$n$.
\end{lemma}
\begin{proof}
Suppose $x \in P$, as must exist since $P$ is nonempty. Let $A^=$ be the constraints (possibly 0 of them) which are tight for $x$, i.e., $A^=x = b^=$. Two cases:

\begin{itemize}
\item $A^=$ is rank-n. Then some size-$n$ subset of its rows is also 
\item $A^=$ is rank deficient. Then by the rank-nullity theorem it has a
nontrivial nullspace. Let $z$ be in the nullspace, and without loss of
generality suppose $z_j < 0$ for some $j$. Then consider the ray $x + \lambda
z$ for $\lambda \geq 0$. This ray must leave the polyhedron, because it leaves
the positive orthant when $\lambda = \frac{x_j}{-z_j}$. And along this entire
ray, the equality constraints $A^=$ remain satisfied, since $A^=(x + \lambda
z) = Ax + \lambda Az = Ax = b^=$. So, when it leaves the polyhedron at some
$\lambda_0$, this must be where some new constraint becomes tight. Let $x' = x
+ \lambda_0 z$, and repeat this process from this new $x$.
\end{itemize}

This process can only terminante when $A^=$ is full rank, but it must terminate
because at each step we find a newly tight constraint, and never drop tight
constraints. So, when we terminate, $A^=$ is rank-$n$.
 
\end{proof}

We can first observe that we only need to prove the forward direction
($P$ is non-empty implies that $P \cap [0, R]^n$ is non-empty),
since the reverse is trivial. 

Given the above lemma,
there is a feasible solution $x^*$ that is tight in at least $n$
of the constraints. Let $A^*$ be the $n \times n$ matrix from the rows
of $A$ corresponding to $n$ such tight constraints (chosen arbitrarily to be
rank $n$ if there are more than $n$ tight constraints), and let $b^*$ be the
column vector from the corresponding rows of $b$.  Then by construction the
following equations are satisfied:

\begin{align}
  A^* x^* &= b^* \\
  x^* &= (A^*)^{-1} b^* 
\end{align}

Now from the second equation above we can conclude that each element
of $x^*$ can be written as the sum of $n$ entries from \astarinv,
each multiplied by an integer in the range of $[-U, U]$. Since $A^*$
is an $n \times n$ matrix satisfying the conditions in the premise
of part (1a), we can conclude that the absolute values of the
entries in \astarinv are bounded above by $(nU)^n$. Therefore it
clearly follows that each entry in $x^*$ is bounded above by
$(nU)^n U n = (nU)^{n + 1}$.

Therefore if we set
$R = (nU)^{n + 1}$ it must be the case that if $P$ is
non-empty there is a solution in $P \cap [0, R]^n$.
Finally, we can observe that $\log(R) = (n + 1)\log(n) + (n + 1)\log(U)$
which is clearly in $poly(n\log(U))$, so we are done.


\subsection*{1d}
\subsection*{1e}\begin{algorithm}[H]
\caption{{\sc CheckFeasibility}}
\begin{algorithmic}[1]
\State Let {\em Separation Oracle} check $x\in P_{\epsilon}$
\If{$x \in P $}
\Return nothing
\Else
\State Let $a_j^{\intercal}$ denote the row vector returned by {\em Separation Oracle}
\State \Return $a_j^{\intercal}x = b_j$
\EndIf
\end{algorithmic}
\end{algorithm}
\textbf{Claim:} $P\cap \{x\in\mathbb{R}^n\mid a_j^{\intercal}x = b_j\}$ is non-empty.
\begin{proof}
  Define a new polyhedron $P' = P\cap \{x\in\mathbb{R}^n\mid a_j^{\intercal}x = b_j\} =
  \{x\in\mathbb{R}^n\mid A'x \leq b, a_j^{\intercal}x \leq b_j, -a_j^{\intercal}x \leq -b_j \}$,
  where $A'$ is matrix $A$ without row vector $a_j^{\intercal}$.
  In addition define $P'_{\epsilon} = \{A'x\leq b + \vec{1}\epsilon, a_j^{\intercal}x \leq b_j
  + \epsilon, -a_j^{\intercal}x \leq -b_j + \epsilon\}$ where $\epsilon$ is defined as in the
  solution to (1d) for $P$ (rather than the epsilon defined for $P'$).

  Consider the vector $x$ under interest that was given to the {\em separation oracle}.
  By hypothesis we know that $x \in P_\epsilon$, so it must be the case that $x \in P'_\epsilon$
  also, since it contains the same constraints as $P_\epsilon$ in addition to
  $-a_j^{\intercal}x \leq -b_j + \epsilon$,
  which is clearly satisfied since we know $a_j^{\intercal}x \geq b_j$. This means that
  $P'_\epsilon$ is non-empty. Therefore it follows that $P'$ is non-empty, as required.
\end{proof}
\subsection*{1f}
%My idea for 1f is that we recursively use the algorithm in 1e, and every time
%either we find a point $x$ in $P$ or we can successfully decrease the dimension of the polyhedron by 1
%The part of the problem that I have not figured out is how to determine a
%point $x\in P_{\epsilon}$ to test. In Ellipsoid algorithm, the algorithm recursively
%takes the center of the new ellipse. I think it should work if we recursively test the center of the "cube" in the problem.
%Another aspect that we might need to touch on is how to reshape/scale to get a new
%"cube" after each iteration, which i personally don't think will be super hard.
%Should be similar to the shrink/stretch method for ellipsoid algorithm



\section*{Problem 2}
\subsection*{2a}


In order to deal with the case where the LP is potentially unbounded, we can simply,
while the problem is unbounded, maintain in addition to some feasible $x$, a direction $w$ such that $x + tw$ is feasible for all $t \in \mathbb{R}_{\geq0}$ and $w \cdot c > 0$.

Initially, before any constraints have been processed, we set the weight vector $w$ to $c$.

When we process a new constraint $a_i x \leq b_i$ we check what this does to the ray $x + tw$. There are 4 cases:

\begin{itemize}
\item The entire ray already meets this constraint. In this case, we can continue to the next constraint.
\item The point $x$ violates this constraint, but at some $t_0$ the ray becomes feasible and stays feasible. Then we update $x \leftarrow x + t_0 w$.
\item There is some $t_0$ (possibly 0) such that the entire ray $x + t w$ for $t\geq t_0$ is infeasible. Then $x$ itself is also not feasible, so just as in the problem-statement algorithm, we recurse to find a new $x$ (and possibly $w$).
\end{itemize}


%If at any point in the algorithm
%$w = 0$ and we encounter a variable that is unconstrained
%in the direction of increasing the objective function --- that is, it has a positive
%(negative) coefficient in $c$ and no constraint with a positive (negative) coefficient
%--- then we set the value of $w$ for exactly one of these variables to $1$ ($-1$).
%
%When we check for constraint violations, we must take into account the $w$ vector.
%In addition to checking whether $x$ violates each constraint directly, for each
%constraint \constraintj we check whether $w$ violates the constraint. This is done
%by checking whether $a_j^T w > 0$: if this is the case then the constraint will
%become violated as $t \rightarrow \infty$. If a constraint is violated in either
%of these ways then we use that constraint to eliminate a variable and recursively
%solve a smaller LP as usual.
%
%The final complication for this method comes when we solve a subproblem with an
%unbounded solution ($w \neq 0$), and deciding how to set the value of $w$ for the
%new solution to the problem with the eliminated variable and constraint re-introduced.
%Suppose that when we constructed the subproblem we eliminated variable $x_k$
%according to the equation $x_k = d_0 + \sum_{j \neq k}{d_j x_j}$, and let
%$\{w_j | j \neq k\}$ denote the weights in the solution to the subproblem.
%Then the weight of $x_k$ in the new solution with $x_k$ re-introduced should
%be set to $w_k = \sum_{j \neq k}{d_j w_j}$.
%
%Given this extra bookkeeping and methods for checking constraints and using
%the solution to subproblems we have a concrete algorithm that can deal with
%unbounded solutions. At termination of the algorithm if $w = 0$ we can conclude
%that the problem is bounded, whereas if $w \neq 0$ the problem is unbounded.


\subsection*{2b}



\end{document}
