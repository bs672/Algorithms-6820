\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm, algpseudocode}
\usepackage{enumitem}
\newtheorem{lemma}{Lemma}
\begin{document}
\begin{flushright}
Andrew Bennett\\
Richard Strong Bowen\\
Bhai Jaiveer Singh\\
Ivy Suiwen Wu\\
CS 6820 Problem Set 3
\end{flushright}
\section*{Problem 1}
\subsection*{1a}
\begin{lemma}
For an invertible $n\times n$ matrix $T$ with integral entries from $[-U, U]$,
$\text{det}(T)$ is an integer and $|\text{det}(T)| \leq (nU)^n$
\end{lemma}
\begin{proof}
We argue by induction. For a $1\times 1$ matrix $T = [u]$, we have
$|\text{det}(T)|= |u|\leq U$ and $|\text{det}(T)| = |u|$ which is an integer.
       
For the induction step, let $T$ be $(n+1)\times (n+1)$.
       
Then to prove the upper bound:
\begin{align*}
	\text{det}(T) &= \sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i} \quad \text{(Definition of determinant)}\\
	\left |\text{det}(T)\right |&\leq \left |\sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i}\right|\\
	&\leq \sum_{i=1}^{n}(nU)^{n}\cdot |T_{1i}|\quad \text{By induction hypothesis)}\\
&\leq(n+1)U\cdot (nU)^{n}\\
&\leq ((n+1)U)^{n+1}
\end{align*}
where $M_i$ is the $(1,i)$ matrix minor: the $n\times n$ matrix formed by removing row $1$ and column $i$ of $T$, which is itself integral.

To prove integrality note that
\[\text{det}(T) = \sum_{i=1}^{n+1} (-1)^{i-1}\text{det}(M_i)\cdot T_{1i}\]
expresses $T$ as the sum of products determinants of integral matrices --
integral by the induction hypothesis -- and elements of $T$, also
integral. Hence it is itself integral.

\end{proof}

Cramer's rule (todo: cite Strang) tells us that 
\[T^{-1} = \frac{1}{\text{det}(T)}C^{\intercal},\]
where $C$ is the cofactor matrix of $T$. Every element of $C$ is the
determinant of some matrix minor of $T$, which are all invertible integral
matrices. Therefore, the lemma tells us that the absolute value of elements of $C$ is bounded
between $[1, (nU)^{n}]$. From the lemma above, we also have
$\frac{1}{|\text{det}(T)|}$ is bounded between $[(nU)^{-n},1]$. This concludes
the proof that the absolute value of entries of $T^{-1}$ is bounded above by
$(nU)^{n}$ and the absolute value of non-zero entries of $T^{-1}$ is bounded
below by $(nU)^{-n}$.
\subsection*{1b}
Let $P = \{x \in \mathbb{R}^n | Ax \leqslant b\}$. We will construct $Q = \{y \in \mathbb{R}^{2n} | A'x \leqslant b'\} $ such that $Q$ is feasible if and only if $P$ is, by constructing $A'$ and $b'$. Let
\[ A' = \begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix}\]
(where $I_k$ means the $k \times k$ identity matrix) and
\[ b' = \begin{bmatrix} b \\ \vec0_{2n} \end{bmatrix} \] 
(where $\vec 0_k$ means the $k$-element all-zeros column vector). This transform is clearly polynomial-time, and ensures $Q$ is a subset of the positive orthant because of the last $2n$ constraints.

First we show that $P$ being feasible implies that $Q$ is. Suppose $x \in P$. We will produce a $y\in \mathbb{R}^{2n}$ which is in $Q$. Index $y$ as:
\[ y = \begin{bmatrix} y^+ \\ y^- \end{bmatrix} = \begin{bmatrix} y^+_1 \\ \vdots \\ y^+_n \\ y^-_1 \\ \vdots \\ y^-_n \end{bmatrix}\]
and set
\[y^+_i = \begin{cases} x_i & \quad \text {$x_i$ is nonnegative} \\ 0 & \quad \text {otherwise} \end{cases}\]
and
\[y^-_i = \begin{cases} -x_i & \quad \text {$x_i$ is negative} \\ 0 & \quad \text {otherwise} \end{cases}.\]
Note that $y \geqslant 0$ and $x = y^+ - y^-$. Then
\begin{align*}
A'y &= \begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix} \begin{bmatrix} y^+ \\ y^- \end{bmatrix} \\
&= \begin{bmatrix} Ax \\ -y \end{bmatrix} \\
&\leqslant \begin{bmatrix} b \\ \vec{0}_{2n} \end{bmatrix} \quad \text {(since $x \in P$ and $y$ is nonnegative)}\\
&= b'
\end{align*}
Hence $y \in Q$.

Second we show that $Q$ being feasible imples that $P$ is. Suppose $y\in Q$. We will produce an $x \in \mathbb{R}^{2n}$ which is in $P$. Index $y$ as above and let $x = y^+ - y^-$. Then we have this chain of implications
\begin{align*}
&\begin{bmatrix} A & -A \\ \multicolumn{2}{c}{-I_{2n}} \end{bmatrix} \begin{bmatrix} y^+ \\ y^- \end{bmatrix} \leqslant b' \quad \text{(since $y \in Q$)}\\
\implies & A(y^+ - y^-) \leqslant b \quad \text{(Ignoring the last $2n$ rows)}\\
\implies & Ax \leqslant b
\end{align*}
Hence $x \in Q$.

\subsection*{1c}

\begin{lemma}
  Suppose the polyhedron $P = \{x \in \mathbb{R}^n | Ax \leq b\}$ is non-empty,
  and there are at least as many constraints as variables. Then
  there is a solution $x \in P$ that is tight in at least $n$
  constraints, where $n$ is the number of variables.
\end{lemma}
\begin{proof}
  First we can observe that if $P$ is non-empty there is obviously
  a solution that makes at least one constraint tight (assuming there
  is at least one constraint), because from any feasible solution we
  can move towards the decision boundary defined by any arbitrary
  constraint until we reach it (or alternatively reach the decision
  boundary of some other constraint first.) This gives a
  solution that is tight in one constraint.

  Given this factoid we can show the general result via induction
  on $n$: the case where $n = 1$ follows trivially from the prior
  factoid. Otherwise, as argued above, we can obtain a feasible
  solution where at least one constraint is tight. We can use
  the equation corresponding to this tight constraint to eliminate
  one of the variables, giving a smaller feasibility problem with
  one fewer variable and one fewer constraint. Note that by
  construction this smaller problem is also non-empty, since at minimum
  it contains the above feasible solution. By the inductive hypothesis
  we can find a feasible solution to this sub-problem where $n - 1$
  constraints are tight, which trivially gives a feasible solution
  to the original feasibility problem with $n$ constraints tight
  (by re-introducing the eliminated constraint, and setting the
  eliminated variable according to that constraint). QED.
\end{proof}

% We can first observe that we only need to prove the forward direction
% ($P$ is non-empty implies that $P \cap \[0, R\]^n$ is non-empty),
% since the reverse is trivial. Suppose that $P$ is non-empty. First
% we can observe we can trivially ensure we have at least $n$ constraints
% without changing $P$, by making sure that the non-negativity constraints
% for all variables are explicitly included.

% Given the above lemma,
% there is a feasible solution $x^*$ that is tight in at least $n$
% of the constraints. Let $A^*$ be the $n \times n$ matrix from the rows
% of $A$ corresponding to $n$ such tight constraints (chosen
% arbitrarily if there are more than $n$ tight constraints), and let
% $b^*$ be the column vector from the corresponding rows of $b$.
% Then by construction the following equations are satisifed:


\subsection*{1d}
\subsection*{1e}\begin{algorithm}[H]
\caption{{\sc CheckFeasibility}}
\begin{algorithmic}[1]
\State Let {\em Separation Oracle} check $x\in P_{\epsilon}$
\If{$x \in P $}
\Return nothing
\Else
\State Let $a_j^{\intercal}$ denote the row vector returned by {\em Separation Oracle}
\State \Return $a_j^{\intercal}x = b_j$
\EndIf
\end{algorithmic}
\end{algorithm}
\textbf{Claim:} $P\cap \{x\in\mathbb{R}^n\mid a_j^{\intercal}x = b_j\}$ is non-empty.
\begin{proof}
  Define new polyhedron $P' = P\cap \{x\in\mathbb{R}^n\mid a_j^{\intercal}x = b_j\} =
  \{x\in\mathbb{R}^n\mid A'x \leq b, a_j^{\intercal}x \leq b_j, -a_j^{\intercal}x \leq -b_j \}$,
  where $A'$ is matrix $A$ without row vector $a_j^{\intercal}$ and its corresponding $P'_{\epsilon} = \{A'x\leq b + \vec{1}\epsilon, a_j^{\intercal}x \leq b_j + \epsilon, -a_j^{\intercal}x \leq -b_j + \epsilon\}$ where $\epsilon$ is defined in (1d) for $P$. \\
  Consider the vector $x$ under interest, $x\in P_{\epsilon}$ and {\em separation oracle}
  has verified that $x\notin P$ and outputted row vector $a_j^{\intercal}$ such that $a_j^{\intercal}x > b_j$. Therefore, $x\in P'_{\epsilon}$. According to (1d), since $P'_{\epsilon}$ is non-empty, we conclude that $P'$ is also non-empty. 
\end{proof}
\subsection*{1f}
%My idea for 1f is that we recursively use the algorithm in 1e, and every time
%either we find a point $x$ in $P$ or we can successfully decrease the dimension of the polyhedron by 1
%The part of the problem that I have not figured out is how to determine a
%point $x\in P_{\epsilon}$ to test. In Ellipsoid algorithm, the algorithm recursively
%takes the center of the new ellipse. I think it should work if we recursively test the center of the "cube" in the problem.
%Another aspect that we might need to touch on is how to reshape/scale to get a new
%"cube" after each iteration, which i personally don't think will be super hard.
%Should be similar to the shrink/stretch method for ellipsoid algorithm
\section*{Problem 2}
\subsection*{2a}
\subsection*{2b}
\end{document}
